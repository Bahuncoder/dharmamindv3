"""
üöÄ Performance Benchmarks for DharmaMind Platform
===============================================

Comprehensive performance benchmark suite for the DharmaMind spiritual guidance platform:
- Response time benchmarks across all modules
- Throughput and scalability testing
- Memory and resource utilization analysis
- Cache performance optimization validation
- Concurrent user load testing
- Spiritual guidance quality vs performance tradeoffs

These benchmarks ensure the platform delivers enlightening experiences
at lightning speed while maintaining accuracy and depth.
"""

import pytest
import asyncio
import time
import psutil
import statistics
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import json
import gc

# Import test framework
from tests import (
    test_framework, DharmaMindTestFramework,
    spiritual_test_data, user_test_data,
    test_cache_manager, test_knowledge_cache, test_emotional_cache
)

@dataclass
class PerformanceBenchmark:
    """Performance benchmark result structure"""
    benchmark_name: str
    category: str
    target_metric: float
    actual_metric: float
    units: str
    passed: bool
    details: Dict[str, Any]
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

@dataclass
class ResourceUsage:
    """System resource usage measurement"""
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    disk_io_read: int
    disk_io_write: int
    network_sent: int
    network_recv: int
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()

class DharmaMindPerformanceBenchmarks:
    """Performance benchmark suite for DharmaMind platform"""
    
    def __init__(self):
        self.benchmarks: List[PerformanceBenchmark] = []
        self.resource_measurements: List[ResourceUsage] = []
        
        # Performance targets
        self.performance_targets = {
            'emotional_analysis_time': 0.05,  # 50ms
            'knowledge_search_time': 0.05,    # 50ms
            'complete_guidance_time': 0.2,    # 200ms
            'cache_operation_time': 0.01,     # 10ms
            'concurrent_user_capacity': 50,   # users
            'queries_per_second': 100,        # QPS
            'memory_usage_mb': 512,           # MB
            'cpu_usage_percent': 70,          # %
            'cache_hit_ratio': 0.85,          # 85%
            'p95_response_time': 0.3,         # 300ms
            'p99_response_time': 0.5          # 500ms
        }
    
    async def run_all_benchmarks(self, test_emotional_cache, test_knowledge_cache, test_cache_manager) -> Dict[str, Any]:
        """Run complete performance benchmark suite"""
        logger = test_framework.test_results
        
        print("üöÄ Starting DharmaMind Performance Benchmarks")
        print("=" * 60)
        
        start_time = time.time()
        
        try:
            # Core module benchmarks
            await self._benchmark_emotional_analysis(test_emotional_cache)
            await self._benchmark_knowledge_search(test_knowledge_cache)
            await self._benchmark_cache_operations(test_cache_manager)
            await self._benchmark_complete_guidance_workflow(test_emotional_cache, test_knowledge_cache)
            
            # Scalability benchmarks
            await self._benchmark_concurrent_operations(test_emotional_cache, test_knowledge_cache)
            await self._benchmark_sustained_load(test_emotional_cache, test_knowledge_cache)
            await self._benchmark_memory_efficiency(test_emotional_cache, test_knowledge_cache, test_cache_manager)
            
            # Advanced benchmarks
            await self._benchmark_cache_performance(test_cache_manager)
            await self._benchmark_response_time_distribution(test_emotional_cache, test_knowledge_cache)
            
            total_time = time.time() - start_time
            
            # Generate comprehensive report
            report = self._generate_benchmark_report(total_time)
            
            print("‚úÖ Performance benchmarks completed!")
            print(f"Total benchmark time: {total_time:.2f}s")
            
            return report
            
        except Exception as e:
            print(f"‚ùå Benchmark suite failed: {e}")
            return {"error": str(e), "status": "failed"}
    
    async def _benchmark_emotional_analysis(self, test_emotional_cache):
        """Benchmark emotional analysis performance"""
        async with test_framework.test_context("emotional_analysis_benchmark", "performance") as test_result:
            print("üß† Benchmarking emotional analysis performance...")
            
            # Test queries with varying complexity
            test_queries = [
                "I seek peace",  # Simple
                "I am struggling with anger and need spiritual guidance to find inner peace",  # Medium
                "I have been practicing meditation for months but still feel disconnected from my true self and wonder if I am making progress on my spiritual journey toward enlightenment",  # Complex
                "How can I integrate mindfulness, compassion, and wisdom teachings into my daily life while managing work stress and family responsibilities?",  # Very complex
                "‡§Æ‡•à‡§Ç ‡§∂‡§æ‡§Ç‡§§‡§ø ‡§ö‡§æ‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç"  # Non-English
            ]
            
            response_times = []
            confidence_scores = []
            
            # Warmup
            await test_emotional_cache.analyze_emotional_state("warmup query", "benchmark_user")
            
            # Benchmark each query type
            for i, query in enumerate(test_queries):
                query_times = []
                
                # Run multiple iterations for statistical significance
                for iteration in range(10):
                    start_time = time.time()
                    
                    analysis = await test_emotional_cache.analyze_emotional_state(
                        query, f"benchmark_user_{i}_{iteration}"
                    )
                    
                    query_time = time.time() - start_time
                    query_times.append(query_time)
                    
                    confidence_scores.append(analysis.get('confidence', 0))
                
                # Statistical analysis
                avg_time = statistics.mean(query_times)
                median_time = statistics.median(query_times)
                p95_time = statistics.quantiles(query_times, n=20)[18]  # 95th percentile
                
                response_times.extend(query_times)
                
                print(f"  Query {i+1}: {avg_time:.3f}s avg, {p95_time:.3f}s P95")
            
            # Overall statistics
            overall_avg = statistics.mean(response_times)
            overall_p95 = statistics.quantiles(response_times, n=20)[18]
            avg_confidence = statistics.mean(confidence_scores)
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="emotional_analysis_time",
                category="core_modules",
                target_metric=self.performance_targets['emotional_analysis_time'],
                actual_metric=overall_avg,
                units="seconds",
                passed=overall_avg <= self.performance_targets['emotional_analysis_time'],
                details={
                    'queries_tested': len(test_queries),
                    'iterations_per_query': 10,
                    'avg_time': overall_avg,
                    'p95_time': overall_p95,
                    'avg_confidence': avg_confidence,
                    'response_times': response_times
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {overall_avg:.3f}s avg (target: {self.performance_targets['emotional_analysis_time']:.3f}s)")
    
    async def _benchmark_knowledge_search(self, test_knowledge_cache):
        """Benchmark knowledge search performance"""
        async with test_framework.test_context("knowledge_search_benchmark", "performance") as test_result:
            print("üìö Benchmarking knowledge search performance...")
            
            # Test queries with different complexity and topics
            search_queries = [
                "meditation",  # Single term
                "inner peace and tranquility",  # Multiple terms
                "What is the nature of consciousness in Vedantic philosophy?",  # Complex philosophical
                "How to practice mindfulness meditation for emotional regulation?",  # Practical guidance
                "Explain the relationship between dharma, karma, and moksha"  # Interconnected concepts
            ]
            
            search_times = []
            concepts_found = []
            cache_hits = 0
            
            # Warmup
            await test_knowledge_cache.search_spiritual_knowledge("warmup", "benchmark_user")
            
            for i, query in enumerate(search_queries):
                query_times = []
                
                for iteration in range(8):
                    start_time = time.time()
                    
                    results = await test_knowledge_cache.search_spiritual_knowledge(
                        query, f"search_benchmark_user_{i}_{iteration}"
                    )
                    
                    search_time = time.time() - start_time
                    query_times.append(search_time)
                    
                    # Track results quality
                    concepts_found.append(len(results.get('concepts', [])))
                    
                    # Check for cache hit (simplified detection)
                    if search_time < 0.02:  # Very fast responses likely cached
                        cache_hits += 1
                
                search_times.extend(query_times)
                avg_time = statistics.mean(query_times)
                print(f"  Query {i+1}: {avg_time:.3f}s avg")
            
            # Overall statistics
            overall_avg = statistics.mean(search_times)
            overall_p95 = statistics.quantiles(search_times, n=20)[18]
            avg_concepts = statistics.mean(concepts_found)
            cache_hit_ratio = cache_hits / len(search_times)
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="knowledge_search_time",
                category="core_modules",
                target_metric=self.performance_targets['knowledge_search_time'],
                actual_metric=overall_avg,
                units="seconds",
                passed=overall_avg <= self.performance_targets['knowledge_search_time'],
                details={
                    'queries_tested': len(search_queries),
                    'iterations_per_query': 8,
                    'avg_time': overall_avg,
                    'p95_time': overall_p95,
                    'avg_concepts_found': avg_concepts,
                    'cache_hit_ratio': cache_hit_ratio,
                    'search_times': search_times
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {overall_avg:.3f}s avg (target: {self.performance_targets['knowledge_search_time']:.3f}s)")
    
    async def _benchmark_cache_operations(self, test_cache_manager):
        """Benchmark cache operation performance"""
        async with test_framework.test_context("cache_operations_benchmark", "performance") as test_result:
            print("üóÑÔ∏è Benchmarking cache operations...")
            
            from app.services.cache_service import CacheCategory
            
            # Test different cache operations
            set_times = []
            get_times = []
            delete_times = []
            
            # Test data of various sizes
            test_data_sizes = [
                ("small", {"key": "value"}),
                ("medium", {"data": "x" * 1000, "metadata": {"type": "test"}}),
                ("large", {"content": "x" * 10000, "details": list(range(100))})
            ]
            
            for size_name, test_data in test_data_sizes:
                for i in range(20):
                    cache_key = f"benchmark_{size_name}_{i}"
                    
                    # Benchmark SET operation
                    start_time = time.time()
                    await test_cache_manager.set(CacheCategory.KNOWLEDGE_BASE, cache_key, test_data)
                    set_time = time.time() - start_time
                    set_times.append(set_time)
                    
                    # Benchmark GET operation
                    start_time = time.time()
                    retrieved = await test_cache_manager.get(CacheCategory.KNOWLEDGE_BASE, cache_key)
                    get_time = time.time() - start_time
                    get_times.append(get_time)
                    
                    # Verify data integrity
                    assert retrieved == test_data, f"Data corruption for {size_name} data"
                    
                    # Benchmark DELETE operation
                    start_time = time.time()
                    await test_cache_manager.delete(CacheCategory.KNOWLEDGE_BASE, cache_key)
                    delete_time = time.time() - start_time
                    delete_times.append(delete_time)
            
            # Calculate statistics
            avg_set_time = statistics.mean(set_times)
            avg_get_time = statistics.mean(get_times)
            avg_delete_time = statistics.mean(delete_times)
            overall_avg = statistics.mean(set_times + get_times + delete_times)
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="cache_operation_time",
                category="infrastructure",
                target_metric=self.performance_targets['cache_operation_time'],
                actual_metric=overall_avg,
                units="seconds",
                passed=overall_avg <= self.performance_targets['cache_operation_time'],
                details={
                    'operations_tested': len(set_times) + len(get_times) + len(delete_times),
                    'avg_set_time': avg_set_time,
                    'avg_get_time': avg_get_time,
                    'avg_delete_time': avg_delete_time,
                    'overall_avg': overall_avg,
                    'data_sizes_tested': len(test_data_sizes)
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {overall_avg:.4f}s avg (target: {self.performance_targets['cache_operation_time']:.4f}s)")
    
    async def _benchmark_complete_guidance_workflow(self, test_emotional_cache, test_knowledge_cache):
        """Benchmark complete spiritual guidance workflow"""
        async with test_framework.test_context("complete_guidance_benchmark", "performance") as test_result:
            print("üîÆ Benchmarking complete guidance workflow...")
            
            # Realistic user queries
            guidance_queries = [
                "I am feeling lost and need spiritual direction",
                "How can I overcome anxiety through spiritual practice?",
                "I want to deepen my meditation but feel stuck",
                "What is the meaning of suffering in spiritual growth?",
                "How do I balance worldly duties with spiritual aspiration?"
            ]
            
            workflow_times = []
            guidance_quality_scores = []
            
            for i, query in enumerate(guidance_queries):
                for iteration in range(5):
                    workflow_start = time.time()
                    user_id = f"guidance_benchmark_{i}_{iteration}"
                    
                    # Complete workflow
                    # Step 1: Emotional analysis
                    emotional_analysis = await test_emotional_cache.analyze_emotional_state(query, user_id)
                    
                    # Step 2: Knowledge search
                    knowledge_results = await test_knowledge_cache.search_spiritual_knowledge(
                        query, user_id, emotional_analysis.get('emotional_states', [])
                    )
                    
                    # Step 3: Response generation
                    response = await test_emotional_cache.generate_empathetic_response(
                        emotional_analysis, user_id
                    )
                    
                    workflow_time = time.time() - workflow_start
                    workflow_times.append(workflow_time)
                    
                    # Assess guidance quality
                    quality_score = self._assess_guidance_quality(
                        emotional_analysis, knowledge_results, response
                    )
                    guidance_quality_scores.append(quality_score)
            
            # Statistics
            avg_workflow_time = statistics.mean(workflow_times)
            p95_workflow_time = statistics.quantiles(workflow_times, n=20)[18]
            avg_quality_score = statistics.mean(guidance_quality_scores)
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="complete_guidance_time",
                category="end_to_end",
                target_metric=self.performance_targets['complete_guidance_time'],
                actual_metric=avg_workflow_time,
                units="seconds",
                passed=avg_workflow_time <= self.performance_targets['complete_guidance_time'],
                details={
                    'queries_tested': len(guidance_queries),
                    'iterations_per_query': 5,
                    'avg_workflow_time': avg_workflow_time,
                    'p95_workflow_time': p95_workflow_time,
                    'avg_quality_score': avg_quality_score,
                    'workflow_times': workflow_times
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {avg_workflow_time:.3f}s avg (target: {self.performance_targets['complete_guidance_time']:.3f}s)")
    
    async def _benchmark_concurrent_operations(self, test_emotional_cache, test_knowledge_cache):
        """Benchmark concurrent operation capacity"""
        async with test_framework.test_context("concurrent_operations_benchmark", "performance") as test_result:
            print("üë• Benchmarking concurrent operations...")
            
            concurrent_levels = [10, 25, 50, 75, 100]
            concurrent_results = []
            
            for level in concurrent_levels:
                print(f"  Testing {level} concurrent operations...")
                
                async def concurrent_operation(user_index: int):
                    query = f"Concurrent test query for user {user_index}"
                    user_id = f"concurrent_user_{user_index}"
                    
                    start_time = time.time()
                    
                    # Emotional analysis
                    emotional_analysis = await test_emotional_cache.analyze_emotional_state(query, user_id)
                    
                    # Knowledge search  
                    knowledge_results = await test_knowledge_cache.search_spiritual_knowledge(
                        query, user_id, emotional_analysis.get('emotional_states', [])
                    )
                    
                    operation_time = time.time() - start_time
                    return operation_time, emotional_analysis is not None, knowledge_results is not None
                
                # Run concurrent operations
                start_time = time.time()
                tasks = [concurrent_operation(i) for i in range(level)]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                total_time = time.time() - start_time
                
                # Analyze results
                successful_ops = 0
                response_times = []
                errors = 0
                
                for result in results:
                    if isinstance(result, Exception):
                        errors += 1
                    else:
                        op_time, emotional_success, knowledge_success = result
                        if emotional_success and knowledge_success:
                            successful_ops += 1
                            response_times.append(op_time)
                
                success_rate = successful_ops / level
                avg_response_time = statistics.mean(response_times) if response_times else float('inf')
                throughput = successful_ops / total_time
                
                concurrent_results.append({
                    'level': level,
                    'success_rate': success_rate,
                    'avg_response_time': avg_response_time,
                    'throughput': throughput,
                    'errors': errors
                })
                
                print(f"    Success rate: {success_rate:.1%}, Avg time: {avg_response_time:.3f}s")
            
            # Find maximum sustainable concurrent level
            max_sustainable_level = 0
            for result in concurrent_results:
                if result['success_rate'] >= 0.95 and result['avg_response_time'] <= 1.0:
                    max_sustainable_level = result['level']
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="concurrent_user_capacity",
                category="scalability",
                target_metric=self.performance_targets['concurrent_user_capacity'],
                actual_metric=max_sustainable_level,
                units="users",
                passed=max_sustainable_level >= self.performance_targets['concurrent_user_capacity'],
                details={
                    'max_sustainable_level': max_sustainable_level,
                    'levels_tested': concurrent_levels,
                    'concurrent_results': concurrent_results
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {max_sustainable_level} users (target: {self.performance_targets['concurrent_user_capacity']})")
    
    async def _benchmark_sustained_load(self, test_emotional_cache, test_knowledge_cache):
        """Benchmark sustained load performance"""
        async with test_framework.test_context("sustained_load_benchmark", "performance") as test_result:
            print("‚ö° Benchmarking sustained load performance...")
            
            # Test parameters
            duration = 30  # seconds
            target_qps = 20  # Start with moderate load
            
            start_time = time.time()
            completed_operations = 0
            response_times = []
            resource_usage = []
            
            # Monitor resource usage
            async def monitor_resources():
                while time.time() - start_time < duration:
                    process = psutil.Process()
                    cpu_percent = process.cpu_percent()
                    memory_info = process.memory_info()
                    
                    usage = ResourceUsage(
                        cpu_percent=cpu_percent,
                        memory_mb=memory_info.rss / 1024 / 1024,
                        memory_percent=process.memory_percent(),
                        disk_io_read=0,  # Simplified for testing
                        disk_io_write=0,
                        network_sent=0,
                        network_recv=0
                    )
                    resource_usage.append(usage)
                    
                    await asyncio.sleep(1)
            
            # Start resource monitoring
            monitor_task = asyncio.create_task(monitor_resources())
            
            # Sustained load test
            operation_interval = 1.0 / target_qps
            next_operation_time = start_time
            
            while time.time() < start_time + duration:
                if time.time() >= next_operation_time:
                    # Perform operation
                    op_start = time.time()
                    
                    try:
                        query = f"Sustained load query {completed_operations}"
                        user_id = f"load_user_{completed_operations % 100}"
                        
                        emotional_analysis = await test_emotional_cache.analyze_emotional_state(query, user_id)
                        knowledge_results = await test_knowledge_cache.search_spiritual_knowledge(
                            query, user_id, emotional_analysis.get('emotional_states', [])
                        )
                        
                        op_time = time.time() - op_start
                        response_times.append(op_time)
                        completed_operations += 1
                        
                    except Exception as e:
                        print(f"Operation {completed_operations} failed: {e}")
                    
                    next_operation_time += operation_interval
                
                await asyncio.sleep(0.01)  # Small delay to prevent busy waiting
            
            # Stop monitoring
            monitor_task.cancel()
            
            # Calculate metrics
            actual_qps = completed_operations / duration
            avg_response_time = statistics.mean(response_times) if response_times else 0
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else 0
            
            # Resource usage statistics
            if resource_usage:
                avg_cpu = statistics.mean([r.cpu_percent for r in resource_usage])
                avg_memory = statistics.mean([r.memory_mb for r in resource_usage])
                max_memory = max([r.memory_mb for r in resource_usage])
            else:
                avg_cpu = avg_memory = max_memory = 0
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="queries_per_second",
                category="scalability",
                target_metric=self.performance_targets['queries_per_second'],
                actual_metric=actual_qps,
                units="qps",
                passed=actual_qps >= target_qps * 0.9,  # Allow 10% tolerance
                details={
                    'duration': duration,
                    'target_qps': target_qps,
                    'actual_qps': actual_qps,
                    'completed_operations': completed_operations,
                    'avg_response_time': avg_response_time,
                    'p95_response_time': p95_response_time,
                    'avg_cpu_percent': avg_cpu,
                    'avg_memory_mb': avg_memory,
                    'max_memory_mb': max_memory
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {actual_qps:.1f} QPS (target: {target_qps} QPS)")
    
    async def _benchmark_memory_efficiency(self, test_emotional_cache, test_knowledge_cache, test_cache_manager):
        """Benchmark memory efficiency"""
        async with test_framework.test_context("memory_efficiency_benchmark", "performance") as test_result:
            print("üß† Benchmarking memory efficiency...")
            
            # Measure baseline memory
            gc.collect()  # Force garbage collection
            process = psutil.Process()
            baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # Perform memory-intensive operations
            operations_performed = 0
            
            # Cache many items
            for i in range(1000):
                await test_cache_manager.set(
                    "test", f"memory_test_{i}", 
                    {"data": f"test_data_{i}", "index": i}
                )
                operations_performed += 1
            
            # Perform many analyses
            for i in range(100):
                query = f"Memory test query number {i} with some additional content to simulate realistic usage"
                await test_emotional_cache.analyze_emotional_state(query, f"memory_user_{i}")
                await test_knowledge_cache.search_spiritual_knowledge(query, f"memory_user_{i}")
                operations_performed += 2
            
            # Measure peak memory
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = peak_memory - baseline_memory
            memory_per_operation = memory_increase / operations_performed if operations_performed > 0 else 0
            
            # Force cleanup and measure final memory
            gc.collect()
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_retained = final_memory - baseline_memory
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="memory_usage_mb",
                category="efficiency",
                target_metric=self.performance_targets['memory_usage_mb'],
                actual_metric=peak_memory,
                units="MB",
                passed=peak_memory <= self.performance_targets['memory_usage_mb'],
                details={
                    'baseline_memory_mb': baseline_memory,
                    'peak_memory_mb': peak_memory,
                    'final_memory_mb': final_memory,
                    'memory_increase_mb': memory_increase,
                    'memory_retained_mb': memory_retained,
                    'operations_performed': operations_performed,
                    'memory_per_operation_kb': memory_per_operation * 1024
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {peak_memory:.1f} MB peak (target: {self.performance_targets['memory_usage_mb']} MB)")
    
    async def _benchmark_cache_performance(self, test_cache_manager):
        """Benchmark cache-specific performance metrics"""
        async with test_framework.test_context("cache_performance_benchmark", "performance") as test_result:
            print("‚ö° Benchmarking cache performance...")
            
            from app.services.cache_service import CacheCategory
            
            # Simulate realistic cache usage patterns
            cache_operations = 1000
            cache_hits = 0
            cache_misses = 0
            
            # Phase 1: Populate cache (should be all misses)
            for i in range(cache_operations // 2):
                key = f"cache_perf_test_{i % 100}"  # Reuse some keys
                data = {"index": i, "data": f"test_data_{i}"}
                
                # Check if exists (should miss initially)
                existing = await test_cache_manager.get(CacheCategory.KNOWLEDGE_BASE, key)
                if existing is None:
                    cache_misses += 1
                else:
                    cache_hits += 1
                
                # Set data
                await test_cache_manager.set(CacheCategory.KNOWLEDGE_BASE, key, data)
            
            # Phase 2: Mixed reads (should have some hits)
            for i in range(cache_operations // 2):
                key = f"cache_perf_test_{i % 100}"  # Reuse keys for hits
                
                existing = await test_cache_manager.get(CacheCategory.KNOWLEDGE_BASE, key)
                if existing is None:
                    cache_misses += 1
                else:
                    cache_hits += 1
            
            # Calculate cache hit ratio
            total_ops = cache_hits + cache_misses
            cache_hit_ratio = cache_hits / total_ops if total_ops > 0 else 0
            
            # Get cache statistics
            cache_stats = await test_cache_manager.get_cache_statistics()
            
            # Create benchmark
            benchmark = PerformanceBenchmark(
                benchmark_name="cache_hit_ratio",
                category="caching",
                target_metric=self.performance_targets['cache_hit_ratio'],
                actual_metric=cache_hit_ratio,
                units="ratio",
                passed=cache_hit_ratio >= self.performance_targets['cache_hit_ratio'] * 0.8,  # Lower for test
                details={
                    'total_operations': total_ops,
                    'cache_hits': cache_hits,
                    'cache_misses': cache_misses,
                    'cache_hit_ratio': cache_hit_ratio,
                    'cache_statistics': cache_stats
                }
            )
            
            self.benchmarks.append(benchmark)
            test_result.details = asdict(benchmark)
            
            print(f"  üìä Result: {cache_hit_ratio:.1%} hit ratio (target: {self.performance_targets['cache_hit_ratio']:.1%})")
    
    async def _benchmark_response_time_distribution(self, test_emotional_cache, test_knowledge_cache):
        """Benchmark response time distribution and percentiles"""
        async with test_framework.test_context("response_time_distribution_benchmark", "performance") as test_result:
            print("üìä Benchmarking response time distribution...")
            
            # Collect response times for statistical analysis
            all_response_times = []
            
            # Test with varied load patterns
            test_scenarios = [
                ("burst", 50, 0.01),    # Burst: 50 ops with 10ms intervals
                ("steady", 100, 0.1),   # Steady: 100 ops with 100ms intervals
                ("sparse", 30, 0.5),    # Sparse: 30 ops with 500ms intervals
            ]
            
            for scenario_name, num_ops, interval in test_scenarios:
                print(f"  Testing {scenario_name} pattern...")
                
                scenario_times = []
                
                for i in range(num_ops):
                    start_time = time.time()
                    
                    # Randomly choose operation type
                    if i % 2 == 0:
                        # Emotional analysis
                        await test_emotional_cache.analyze_emotional_state(
                            f"{scenario_name} test query {i}", f"dist_user_{scenario_name}_{i}"
                        )
                    else:
                        # Knowledge search
                        await test_knowledge_cache.search_spiritual_knowledge(
                            f"{scenario_name} search {i}", f"dist_user_{scenario_name}_{i}"
                        )
                    
                    response_time = time.time() - start_time
                    scenario_times.append(response_time)
                    all_response_times.append(response_time)
                    
                    if interval > 0:
                        await asyncio.sleep(interval)
                
                # Scenario statistics
                scenario_avg = statistics.mean(scenario_times)
                print(f"    {scenario_name}: {scenario_avg:.3f}s avg")
            
            # Calculate percentiles
            sorted_times = sorted(all_response_times)
            n = len(sorted_times)
            
            percentiles = {
                'p50': sorted_times[int(0.5 * n)],
                'p75': sorted_times[int(0.75 * n)],
                'p90': sorted_times[int(0.9 * n)],
                'p95': sorted_times[int(0.95 * n)],
                'p99': sorted_times[int(0.99 * n)]
            }
            
            avg_time = statistics.mean(all_response_times)
            median_time = statistics.median(all_response_times)
            
            # Create benchmarks for key percentiles
            p95_benchmark = PerformanceBenchmark(
                benchmark_name="p95_response_time",
                category="latency",
                target_metric=self.performance_targets['p95_response_time'],
                actual_metric=percentiles['p95'],
                units="seconds",
                passed=percentiles['p95'] <= self.performance_targets['p95_response_time'],
                details={
                    'total_operations': len(all_response_times),
                    'avg_time': avg_time,
                    'median_time': median_time,
                    'percentiles': percentiles,
                    'scenarios_tested': len(test_scenarios)
                }
            )
            
            p99_benchmark = PerformanceBenchmark(
                benchmark_name="p99_response_time",
                category="latency",
                target_metric=self.performance_targets['p99_response_time'],
                actual_metric=percentiles['p99'],
                units="seconds",
                passed=percentiles['p99'] <= self.performance_targets['p99_response_time'],
                details=p95_benchmark.details
            )
            
            self.benchmarks.extend([p95_benchmark, p99_benchmark])
            test_result.details = asdict(p95_benchmark)
            
            print(f"  üìä Result: P95={percentiles['p95']:.3f}s, P99={percentiles['p99']:.3f}s")
    
    def _assess_guidance_quality(self, emotional_analysis: Dict, knowledge_results: Dict, response: Dict) -> float:
        """Assess the quality of spiritual guidance"""
        score = 0.0
        max_score = 4.0
        
        # Emotional analysis quality (0-1)
        if emotional_analysis.get('confidence', 0) >= 0.7:
            score += 1.0
        elif emotional_analysis.get('confidence', 0) >= 0.5:
            score += 0.5
        
        # Knowledge relevance (0-1)
        concepts = knowledge_results.get('concepts', [])
        practices = knowledge_results.get('recommended_practices', [])
        if len(concepts) >= 3 and len(practices) >= 2:
            score += 1.0
        elif len(concepts) >= 1 and len(practices) >= 1:
            score += 0.5
        
        # Response quality (0-1)
        response_confidence = response.get('confidence', 0)
        if response_confidence >= 0.8:
            score += 1.0
        elif response_confidence >= 0.6:
            score += 0.5
        
        # Integration quality (0-1)
        personalization = response.get('personalization_level', 0)
        if personalization >= 0.7:
            score += 1.0
        elif personalization >= 0.4:
            score += 0.5
        
        return score / max_score
    
    def _generate_benchmark_report(self, total_time: float) -> Dict[str, Any]:
        """Generate comprehensive benchmark report"""
        if not self.benchmarks:
            return {"status": "No benchmarks completed"}
        
        # Calculate summary statistics
        total_benchmarks = len(self.benchmarks)
        passed_benchmarks = sum(1 for b in self.benchmarks if b.passed)
        failed_benchmarks = total_benchmarks - passed_benchmarks
        
        pass_rate = passed_benchmarks / total_benchmarks
        
        # Group benchmarks by category
        categories = {}
        for benchmark in self.benchmarks:
            category = benchmark.category
            if category not in categories:
                categories[category] = []
            categories[category].append(benchmark)
        
        # Category summaries
        category_summaries = {}
        for category, benchmarks in categories.items():
            passed = sum(1 for b in benchmarks if b.passed)
            total = len(benchmarks)
            category_summaries[category] = {
                'total': total,
                'passed': passed,
                'failed': total - passed,
                'pass_rate': passed / total
            }
        
        # Performance insights
        insights = []
        
        # Check if major targets are met
        core_benchmarks = [b for b in self.benchmarks if b.category == "core_modules"]
        if all(b.passed for b in core_benchmarks):
            insights.append("‚úÖ All core module performance targets met")
        else:
            failed_core = [b for b in core_benchmarks if not b.passed]
            insights.append(f"‚ö†Ô∏è {len(failed_core)} core module benchmarks failed")
        
        # Check scalability
        scalability_benchmarks = [b for b in self.benchmarks if b.category == "scalability"]
        if any(b.benchmark_name == "concurrent_user_capacity" and b.passed for b in scalability_benchmarks):
            insights.append("‚úÖ Concurrent user capacity targets met")
        
        # Check cache performance
        cache_benchmarks = [b for b in self.benchmarks if b.category == "caching"]
        if any(b.benchmark_name == "cache_hit_ratio" and b.passed for b in cache_benchmarks):
            insights.append("‚úÖ Cache performance targets met")
        
        return {
            'summary': {
                'total_benchmarks': total_benchmarks,
                'passed_benchmarks': passed_benchmarks,
                'failed_benchmarks': failed_benchmarks,
                'pass_rate': pass_rate,
                'total_benchmark_time': total_time
            },
            'category_summaries': category_summaries,
            'performance_insights': insights,
            'detailed_benchmarks': [asdict(b) for b in self.benchmarks],
            'performance_targets': self.performance_targets,
            'overall_status': 'PASSED' if pass_rate >= 0.8 else 'FAILED',
            'report_generated': datetime.now().isoformat()
        }

# Performance test runner
async def run_performance_benchmarks():
    """Run the complete performance benchmark suite"""
    # Import test fixtures
    import fakeredis.aioredis
    from app.services.cache_service import CacheService as AdvancedCacheManager
    from app.services.intelligent_cache import OptimizedKnowledgeCache
    from app.services.intelligent_cache import OptimizedEmotionalCache
    
    # Create test environment
    fake_redis = fakeredis.aioredis.FakeRedis()
    cache_manager = AdvancedCacheManager(redis_client=fake_redis)
    
    knowledge_cache = OptimizedKnowledgeCache(cache_manager)
    await knowledge_cache.initialize()
    
    emotional_cache = OptimizedEmotionalCache(cache_manager)
    await emotional_cache.initialize()
    
    # Run benchmarks
    benchmark_suite = DharmaMindPerformanceBenchmarks()
    report = await benchmark_suite.run_all_benchmarks(emotional_cache, knowledge_cache, cache_manager)
    
    # Cleanup
    await fake_redis.close()
    
    return report

# Export for use in tests
__all__ = [
    'DharmaMindPerformanceBenchmarks',
    'PerformanceBenchmark',
    'ResourceUsage',
    'run_performance_benchmarks'
]