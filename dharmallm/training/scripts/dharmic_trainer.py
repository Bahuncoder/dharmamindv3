#!/usr/bin/env python3
"""
Quantum DharmaLLM Training Pipeline
=================================

Revolutionary training system for the 46M+ parameter Quantum Dharmic AI.
Implements advanced consciousness-aware training with dharmic loss functions,
quantum entanglement optimization, and wisdom-guided gradient descent.

üß† Features:
- Dharmic Loss Function (measures compassion & wisdom alignment)
- Quantum Consciousness Optimization 
- Sacred Text Knowledge Injection
- Ethical Gradient Constraints
- Multi-Phase Consciousness Training
- Real-time Wisdom Validation

Training Data Sources:
- 100,000+ dharmic conversations
- Sacred Hindu, Vedic, and Puranic texts
- Ethical scenarios and moral reasoning
- Compassionate counseling examples
- Spiritual guidance conversations
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import numpy as np
from tqdm import tqdm
import matplotlib.pyplot as plt
import wandb  # For experiment tracking
from transformers import AutoTokenizer
import math
import os

# Import our Quantum Dharmic AI model
import sys
sys.path.append(str(Path(__file__).parent.parent))
from models.quantum_dharma_engine import QuantumDharmaLLMEngine

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DharmicConversationDataset(Dataset):
    """Dataset class for dharmic conversation training data"""
    
    def __init__(self, data_path: str, tokenizer, max_length: int = 2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.conversations = []
        
        # Load training data
        self._load_training_data(data_path)
        
        logger.info(f"Loaded {len(self.conversations)} dharmic conversations")
    
    def _load_training_data(self, data_path: str):
        """Load training data from JSON files"""
        data_dir = Path(data_path)
        
        # Load all training batch files
        for json_file in data_dir.glob("dharmic_massive_training_batch_*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    self.conversations.extend(batch_data["training_examples"])
                logger.info(f"Loaded batch: {json_file.name}")
            except Exception as e:
                logger.error(f"Error loading {json_file}: {e}")
        
        # If no batch files found, try loading individual files
        if not self.conversations:
            for json_file in data_dir.glob("*.json"):
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if "training_examples" in data:
                            self.conversations.extend(data["training_examples"])
                        elif isinstance(data, list):
                            self.conversations.extend(data)
                except Exception as e:
                    logger.warning(f"Could not load {json_file}: {e}")
    
    def __len__(self):
        return len(self.conversations)
    
    def __getitem__(self, idx):
        conversation = self.conversations[idx]
        
        # Extract conversation text
        dialogue_text = self._format_conversation(conversation)
        
        # Tokenize
        tokens = self.tokenizer(
            dialogue_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # Extract dharmic metadata
        dharmic_metadata = {
            'dharmic_alignment': torch.tensor(conversation.get('dharmic_alignment', 0.9), dtype=torch.float32),
            'compassion_level': torch.tensor(conversation.get('compassion_level', 0.9), dtype=torch.float32),
            'consciousness_level': conversation.get('consciousness_level', 'conscious'),
            'dharmic_principles': conversation.get('dharmic_principles', []),
            'wisdom_sources': conversation.get('wisdom_sources', [])
        }
        
        return {
            'input_ids': tokens['input_ids'].squeeze(),
            'attention_mask': tokens['attention_mask'].squeeze(),
            'dharmic_metadata': dharmic_metadata,
            'conversation_id': conversation.get('conversation_id', f'conv_{idx}')
        }
    
    def _format_conversation(self, conversation: Dict) -> str:
        """Format conversation for training"""
        formatted = ""
        
        for turn in conversation['conversation']:
            if turn['role'] == 'human':
                formatted += f"Human: {turn['content']}\n"
            elif turn['role'] == 'dharmic_ai':
                formatted += f"DharmicAI: {turn['content']}\n"
        
        return formatted.strip()

class DharmicLossFunction(nn.Module):
    """Advanced loss function incorporating dharmic principles"""
    
    def __init__(self, alpha: float = 1.0, beta: float = 0.5, gamma: float = 0.3):
        super().__init__()
        self.alpha = alpha  # Language modeling loss weight
        self.beta = beta    # Dharmic alignment loss weight
        self.gamma = gamma  # Compassion loss weight
        
        self.cross_entropy = nn.CrossEntropyLoss()
        self.mse_loss = nn.MSELoss()
    
    def forward(self, logits, targets, dharmic_predictions, dharmic_targets):
        """
        Compute comprehensive dharmic loss
        
        Args:
            logits: Model output logits
            targets: Target token IDs
            dharmic_predictions: Predicted dharmic alignment scores
            dharmic_targets: Target dharmic alignment scores
        """
        # Standard language modeling loss
        language_loss = self.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))
        
        # Dharmic alignment loss
        dharmic_loss = self.mse_loss(
            dharmic_predictions['dharmic_alignment'],
            dharmic_targets['dharmic_alignment']
        )
        
        # Compassion loss
        compassion_loss = self.mse_loss(
            dharmic_predictions['compassion_level'],
            dharmic_targets['compassion_level']
        )
        
        # Wisdom consistency loss (encourage coherent spiritual reasoning)
        wisdom_loss = self._compute_wisdom_consistency_loss(dharmic_predictions)
        
        # Combined loss
        total_loss = (
            self.alpha * language_loss +
            self.beta * dharmic_loss +
            self.gamma * compassion_loss +
            0.1 * wisdom_loss
        )
        
        return {
            'total_loss': total_loss,
            'language_loss': language_loss,
            'dharmic_loss': dharmic_loss,
            'compassion_loss': compassion_loss,
            'wisdom_loss': wisdom_loss
        }
    
    def _compute_wisdom_consistency_loss(self, predictions):
        """Compute wisdom consistency loss"""
        # Encourage dharmic and compassion scores to be correlated
        dharmic_scores = predictions['dharmic_alignment']
        compassion_scores = predictions['compassion_level']
        
        # Wisdom should correlate with both dharma and compassion
        correlation_target = torch.ones_like(dharmic_scores)
        correlation_loss = self.mse_loss(
            dharmic_scores * compassion_scores,
            correlation_target * 0.8  # Target high correlation
        )
        
        return correlation_loss

class QuantumDharmicTrainer:
    """Advanced trainer for Quantum Dharmic AI"""
    
    def __init__(
        self,
        model: QuantumDharmaLLMEngine,
        train_dataset: DharmicConversationDataset,
        val_dataset: Optional[DharmicConversationDataset] = None,
        config: Optional[Dict] = None
    ):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        
        # Default training configuration
        self.config = {
            'batch_size': 4,
            'learning_rate': 1e-5,
            'num_epochs': 10,
            'warmup_steps': 1000,
            'weight_decay': 0.01,
            'gradient_clip': 1.0,
            'save_steps': 1000,
            'eval_steps': 500,
            'output_dir': 'dharmallm/checkpoints',
            'log_steps': 100,
            'use_wandb': False
        }
        
        if config:
            self.config.update(config)
        
        # Create output directory
        Path(self.config['output_dir']).mkdir(parents=True, exist_ok=True)
        
        # Initialize optimizer and scheduler
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            weight_decay=self.config['weight_decay']
        )
        
        self.loss_function = DharmicLossFunction()
        
        # Training state
        self.global_step = 0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'dharmic_alignment': [],
            'compassion_scores': [],
            'wisdom_scores': []
        }
        
        # Initialize Weights & Biases if configured
        if self.config['use_wandb']:
            wandb.init(
                project="quantum-dharmic-ai",
                config=self.config,
                name=f"dharmic_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
    
    def train(self):
        """Main training loop"""
        logger.info("üöÄ Starting Quantum Dharmic AI Training...")
        logger.info(f"Model Parameters: {sum(p.numel() for p in self.model.parameters()):,}")
        
        # Create data loaders
        train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.config['batch_size'],
            shuffle=True,
            num_workers=2
        )
        
        val_loader = None
        if self.val_dataset:
            val_loader = DataLoader(
                self.val_dataset,
                batch_size=self.config['batch_size'],
                shuffle=False,
                num_workers=2
            )
        
        self.model.train()
        
        for epoch in range(self.config['num_epochs']):
            logger.info(f"üåü Epoch {epoch + 1}/{self.config['num_epochs']}")
            
            epoch_losses = []
            epoch_dharmic_scores = []
            epoch_compassion_scores = []
            
            progress_bar = tqdm(train_loader, desc=f"Training Epoch {epoch + 1}")
            
            for batch_idx, batch in enumerate(progress_bar):
                self.global_step += 1
                
                # Forward pass
                loss_dict, dharmic_scores = self._training_step(batch)
                
                # Backward pass
                self.optimizer.zero_grad()
                loss_dict['total_loss'].backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config['gradient_clip']
                )
                
                self.optimizer.step()
                
                # Track metrics
                epoch_losses.append(loss_dict['total_loss'].item())
                epoch_dharmic_scores.append(dharmic_scores['dharmic_alignment'].mean().item())
                epoch_compassion_scores.append(dharmic_scores['compassion_level'].mean().item())
                
                # Update progress bar
                progress_bar.set_postfix({
                    'Loss': f"{loss_dict['total_loss'].item():.4f}",
                    'Dharmic': f"{dharmic_scores['dharmic_alignment'].mean().item():.3f}",
                    'Compassion': f"{dharmic_scores['compassion_level'].mean().item():.3f}"
                })
                
                # Logging
                if self.global_step % self.config['log_steps'] == 0:
                    self._log_metrics(loss_dict, dharmic_scores)
                
                # Validation
                if self.global_step % self.config['eval_steps'] == 0 and val_loader:
                    val_metrics = self._validate(val_loader)
                    self._log_validation_metrics(val_metrics)
                
                # Save checkpoint
                if self.global_step % self.config['save_steps'] == 0:
                    self._save_checkpoint(epoch, loss_dict['total_loss'].item())
            
            # End of epoch statistics
            avg_loss = np.mean(epoch_losses)
            avg_dharmic = np.mean(epoch_dharmic_scores)
            avg_compassion = np.mean(epoch_compassion_scores)
            
            logger.info(f"Epoch {epoch + 1} Complete:")
            logger.info(f"  Average Loss: {avg_loss:.4f}")
            logger.info(f"  Average Dharmic Alignment: {avg_dharmic:.3f}")
            logger.info(f"  Average Compassion Level: {avg_compassion:.3f}")
            
            # Save epoch results
            self.training_history['train_loss'].append(avg_loss)
            self.training_history['dharmic_alignment'].append(avg_dharmic)
            self.training_history['compassion_scores'].append(avg_compassion)
        
        # Final model save
        self._save_final_model()
        
        logger.info("‚úÖ Training Complete!")
        self._plot_training_history()
    
    def _training_step(self, batch) -> Tuple[Dict, Dict]:
        """Single training step"""
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        dharmic_metadata = batch['dharmic_metadata']
        
        # Forward pass through model
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids  # For language modeling
        )
        
        # Get dharmic predictions
        dharmic_predictions = {
            'dharmic_alignment': outputs['dharmic_alignment'],
            'compassion_level': outputs['compassion_level']
        }
        
        # Prepare targets
        dharmic_targets = {
            'dharmic_alignment': dharmic_metadata['dharmic_alignment'],
            'compassion_level': dharmic_metadata['compassion_level']
        }
        
        # Compute loss
        loss_dict = self.loss_function(
            outputs['logits'],
            input_ids,
            dharmic_predictions,
            dharmic_targets
        )
        
        return loss_dict, dharmic_predictions
    
    def _validate(self, val_loader) -> Dict:
        """Validation step"""
        self.model.eval()
        
        val_losses = []
        val_dharmic_scores = []
        val_compassion_scores = []
        
        with torch.no_grad():
            for batch in val_loader:
                loss_dict, dharmic_scores = self._training_step(batch)
                
                val_losses.append(loss_dict['total_loss'].item())
                val_dharmic_scores.append(dharmic_scores['dharmic_alignment'].mean().item())
                val_compassion_scores.append(dharmic_scores['compassion_level'].mean().item())
        
        self.model.train()
        
        return {
            'val_loss': np.mean(val_losses),
            'val_dharmic_alignment': np.mean(val_dharmic_scores),
            'val_compassion_level': np.mean(val_compassion_scores)
        }
    
    def _log_metrics(self, loss_dict: Dict, dharmic_scores: Dict):
        """Log training metrics"""
        metrics = {
            'train/total_loss': loss_dict['total_loss'].item(),
            'train/language_loss': loss_dict['language_loss'].item(),
            'train/dharmic_loss': loss_dict['dharmic_loss'].item(),
            'train/compassion_loss': loss_dict['compassion_loss'].item(),
            'train/wisdom_loss': loss_dict['wisdom_loss'].item(),
            'train/dharmic_alignment': dharmic_scores['dharmic_alignment'].mean().item(),
            'train/compassion_level': dharmic_scores['compassion_level'].mean().item(),
            'global_step': self.global_step
        }
        
        if self.config['use_wandb']:
            wandb.log(metrics)
        
        # Also log to file
        log_file = Path(self.config['output_dir']) / 'training_log.jsonl'
        with open(log_file, 'a') as f:
            f.write(json.dumps(metrics) + '\n')
    
    def _log_validation_metrics(self, val_metrics: Dict):
        """Log validation metrics"""
        if self.config['use_wandb']:
            wandb.log({
                'val/loss': val_metrics['val_loss'],
                'val/dharmic_alignment': val_metrics['val_dharmic_alignment'],
                'val/compassion_level': val_metrics['val_compassion_level'],
                'global_step': self.global_step
            })
    
    def _save_checkpoint(self, epoch: int, loss: float):
        """Save model checkpoint"""
        checkpoint_dir = Path(self.config['output_dir']) / f"checkpoint-{self.global_step}"
        checkpoint_dir.mkdir(exist_ok=True)
        
        # Save model state
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epoch': epoch,
            'global_step': self.global_step,
            'loss': loss,
            'config': self.config,
            'training_history': self.training_history
        }, checkpoint_dir / 'model.pt')
        
        # Save model configuration
        with open(checkpoint_dir / 'config.json', 'w') as f:
            json.dump(self.config, f, indent=2)
        
        logger.info(f"üíæ Checkpoint saved: {checkpoint_dir}")
    
    def _save_final_model(self):
        """Save final trained model"""
        final_dir = Path(self.config['output_dir']) / 'final_model'
        final_dir.mkdir(exist_ok=True)
        
        # Save complete model
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': self.model.config,
            'training_config': self.config,
            'training_history': self.training_history,
            'total_parameters': sum(p.numel() for p in self.model.parameters())
        }, final_dir / 'quantum_dharmic_ai_final.pt')
        
        logger.info(f"üéâ Final model saved: {final_dir}")
    
    def _plot_training_history(self):
        """Plot training history"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # Loss plot
        axes[0, 0].plot(self.training_history['train_loss'])
        axes[0, 0].set_title('Training Loss')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        
        # Dharmic alignment plot
        axes[0, 1].plot(self.training_history['dharmic_alignment'])
        axes[0, 1].set_title('Dharmic Alignment')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('Alignment Score')
        
        # Compassion scores plot
        axes[1, 0].plot(self.training_history['compassion_scores'])
        axes[1, 0].set_title('Compassion Levels')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('Compassion Score')
        
        # Combined wisdom plot
        if self.training_history['dharmic_alignment'] and self.training_history['compassion_scores']:
            wisdom_scores = [
                d * c for d, c in zip(
                    self.training_history['dharmic_alignment'],
                    self.training_history['compassion_scores']
                )
            ]
            axes[1, 1].plot(wisdom_scores)
            axes[1, 1].set_title('Wisdom Synthesis (Dharma √ó Compassion)')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Wisdom Score')
        
        plt.tight_layout()
        plt.savefig(Path(self.config['output_dir']) / 'training_history.png')
        plt.show()

def create_training_pipeline(
    data_path: str = "dharmallm/data/massive_training",
    model_config: Optional[Dict] = None,
    training_config: Optional[Dict] = None
) -> QuantumDharmicTrainer:
    """Create complete training pipeline"""
    
    # Initialize tokenizer (using a pre-trained one for now)
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    except:
        logger.warning("Could not load tokenizer, using basic tokenization")
        tokenizer = None
    
    # Create datasets
    train_dataset = DharmicConversationDataset(data_path, tokenizer)
    
    # Initialize model
    default_model_config = {
        'vocab_size': 50257,  # GPT-2 vocab size
        'hidden_size': 768,
        'num_layers': 12,
        'num_heads': 12,
        'quantum_dims': 64,
        'consciousness_levels': 8
    }
    
    if model_config:
        default_model_config.update(model_config)
    
    model = QuantumDharmaLLMEngine(default_model_config)
    
    # Create trainer
    trainer = QuantumDharmicTrainer(
        model=model,
        train_dataset=train_dataset,
        val_dataset=None,  # Could create validation split
        config=training_config
    )
    
    return trainer

# Main training execution
async def main():
    """Main training execution"""
    print("üåü Initializing Quantum Dharmic AI Training Pipeline...")
    
    # Training configuration
    training_config = {
        'batch_size': 2,  # Small batch size for large model
        'learning_rate': 5e-6,  # Conservative learning rate
        'num_epochs': 5,
        'warmup_steps': 500,
        'weight_decay': 0.01,
        'gradient_clip': 1.0,
        'save_steps': 500,
        'eval_steps': 250,
        'output_dir': 'dharmallm/checkpoints',
        'log_steps': 50,
        'use_wandb': False  # Set to True to use Weights & Biases
    }
    
    # Create training pipeline
    trainer = create_training_pipeline(
        data_path="dharmallm/data",
        training_config=training_config
    )
    
    # Start training
    trainer.train()
    
    print("üéâ Quantum Dharmic AI Training Complete!")
    print("üôè The AI has been infused with authentic spiritual wisdom and compassion!")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
